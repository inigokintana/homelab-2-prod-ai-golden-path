homelab-2-prod-ai-golden-path
*******************************

This is a golden path (opinionated shortcut) to explore AI from local homelab to PROD using OSS for technological independence and local LLM for privacy. We explore concepts like AI agents, RAG and LLM using OSS technologies like Linux, Opentofu, microk8s, DAPR, Ollama, TimescaleDB & Vectorizer, Flask and Python.

AI agents RAG LLM Linux microk8s DAPR Ollama TimescaleDB Vectorizer Flask Python Opentofu


1) Infra: linux + mikrok8s  k8s infra services (dapr + TimescaleDB + Vectorizer + Ollama )
	wsl2: shell + kubectl
	aws: opentofu: providers AWS + k8s + helm - minimize shell to have inmutable
	ovh: opentofu: providers AWS + k8s + helm - minimize shell to have inmutable

2) Mandatory k8s services (dapr + TimescaleDB + Vectorizer + Ollama )

3) dapr microservices agents

	3.2) Injection agent
		dapr web ingestion
			k8s cronjob
			scrapera -> vectorized
			
	3.1) User web Flask
			Ollama
			OpenAI
			Save good results + save more details + prometheus
			
			Languages not very well supported
			Honesty - good data quality dependency
			
			Enhance logging Zipkin Prometheus
			
	3.3) Agents and MCP
		https://www.cncf.io/blog/2025/03/12/announcing-dapr-ai-agents/
		Data Science How to build MCP host Ai-agents
		https://github.com/dapr/dapr-agents/issues/50
		https://github.com/dapr/dapr-agents/tree/main/cookbook/mcp
		https://www.diagrid.io/blog/the-agentic-spectrum-why-its-not-agents-vs-workflows
		https://www.diagrid.io/dapr-university#dapr-agents
		
4) Optional k8s services 
	gitOps - ArgoCD
	MLOps - MLFlow or Kubeflow or both
	monitoring- no Dapr
		zipkin
		Prometheus


***
Presentation
	Explain objective
		Easy entry level to AI
		POC 
		Golden path for lab to PROD
		
	Demo
		RAG with Local LLM or OpenAI
		
	Concepts behind this demo
		Why 
			k8s : multiplatform cloud & on-premise - microk8s network not kind
			Dapr:  multiplatform cloud & on-premise - Zipkin & prometheus
			Pg pgai vectorizer: ease data engineers live, batch vs stream ingestion
			Ollama: access to models locally, from small models to big ones
				 be aware <RAG ENCODING> must be supported by OLLAMA
				Flask web where user can write prompt and receive and answer based on local RAG and local Ollama LLM or OpenAI
				Parameters: 
				Honest, PII, 
			Different LLM models we choose OSS encoding and LLM model
			Flask: 
			Tech independance: USA gorila Trump
			tilt: skips docker containers deploy directly to K8S
			Opentofu: immutable state Terraform vs OpenTofu
			
	Install
		Wsl2: c:\Users\<user>\.wslconfig
			memory=10GB processors=4 swap=2GB size=80
		AWS: 12GB RAM
		OVH:
		
	Probable next steps - use cases
		install - Play
		Have your own RAG database - if not in my DB RAG go to OpenAI
		Agents MCP 
		From lab to PROD - Tech change personal view
		gitOps - ArgoCD
		MLOps - MLFlow or Kubeflow or both
		
	Q&A - contribute
		share your experience or thoughts
		any help with your lab
		but do not be a hater
		
**************
LinkedIn - SLM - like 24/jul/2025

SLM
LMs are overkill for 80% of business tasks. Enter SLMs:

Most companies are burning cash on GPT-4 when a specialized Small Language Model would do the job better, faster, and cheaper.

Here's the architecture difference:

Traditional LLMs: Simple linear pipeline that processes everything with maximum resources. Like using a Ferrari for grocery runs.

Smart SLMs: Optimized parallel processing with compact tokenization, task-specific embeddings, and model quantization. Built for edge deployment and real-world efficiency.

Real Cost Comparison:
- GPT-4: $30/million input tokens, $60/million output tokens
- GPT-4.1-nano: $0.10/million input, $0.40/million output (OpenAI's cheapest)
- Llama 3.2 (1B): $0.03-0.05/million tokens
- Custom fine-tuned SLMs can cost even less

Where SLMs Win:
SLMs excel at customer service handling 90% of repetitive queries, document classification, sentiment analysis, code completion for specific languages, and IoT/edge device applications.

Where LLMs Still Rule:
LLMs remain superior for creative writing, complex reasoning tasks, multi-domain applications, and research assistance.

Real Business Case:
Switching from GPT-4 to a specialized SLM for invoice processing:
- Latency: 2s â†’ 0.3s
- Cost: Over 90% reduction
- Accuracy: Improved with domain-specific training

Quick Start Guide:
1. Identify repetitive tasks in your workflow
2. Calculate current LLM costs
3. Test open-source SLMs (Phi-3, TinyLlama, Llama 3.2)
4. Fine-tune on your specific data
5. Deploy locally or on edge

The future isn't about bigger models. It's about smarter, specialized ones that run anywhere.

Over to you: What task are you overpaying LLMs to handle?	