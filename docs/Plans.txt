homelab-2-prod-ai-golden-path
*******************************

This is a golden path (opinionated shortcut) to explore AI from local homelab to PROD using OSS for technological independence and local LLM for privacy. We explore concepts like AI agents, RAG and LLM using OSS technologies like Linux, Opentofu, microk8s, DAPR, Ollama, TimescaleDB & Vectorizer, Flask and Python.

AI agents RAG LLM Linux microk8s DAPR Ollama TimescaleDB Vectorizer Flask Python Opentofu


1) Infra: linux + mikrok8s  k8s infra services (dapr + TimescaleDB + Vectorizer + Ollama )
	wsl2: shell + kubectl
	aws: opentofu: providers AWS + k8s + helm - minimize shell to have inmutable
	ovh: opentofu: providers AWS + k8s + helm - minimize shell to have inmutable

2) Mandatory k8s services (dapr + TimescaleDB + Vectorizer + Ollama )

3) dapr microservices agents

	3.2) Injection agent
		dapr web ingestion
			k8s cronjob
			scrapera -> vectorized
			
	3.1) User web Flask
			Ollama
			OpenAI
			Save good results + save more details + prometheus
			
			Languages not very well supported
			Honesty - good data quality dependency
			
			Enhance logging Zipkin Prometheus
			
	3.3) Agents and MCP
		https://www.cncf.io/blog/2025/03/12/announcing-dapr-ai-agents/
		Data Science How to build MCP host Ai-agents
		https://github.com/dapr/dapr-agents/issues/50
		https://github.com/dapr/dapr-agents/tree/main/cookbook/mcp
		https://www.diagrid.io/blog/the-agentic-spectrum-why-its-not-agents-vs-workflows
		https://www.diagrid.io/dapr-university#dapr-agents
		
4) Optional k8s services 
	gitOps - ArgoCD
	MLOps - MLFlow or Kubeflow or both
	monitoring- no Dapr
		zipkin
		Prometheus


***
Presentation
	Explain objective
		Easy entry level to AI
		POC 
		Golden path for lab to PROD
		
	Demo
		RAG with Local LLM or OpenAI
		
	Concepts behind this demo
		Why 
			k8s : multiplatform cloud & on-premise - microk8s network not kind
			Dapr:  multiplatform cloud & on-premise - Zipkin & prometheus
			Pg pgai vectorizer: ease data engineers live, batch vs stream ingestion
			Ollama: access to models locally, from small models to big ones
				 be aware <RAG ENCODING> must be supported by OLLAMA
				Flask web where user can write prompt and receive and answer based on local RAG and local Ollama LLM or OpenAI
				Parameters: 
				Honest, PII, 
			Different LLM models we choose OSS encoding and LLM model
			Flask: 
			Tech independance: USA gorila Trump
			tilt: skips docker containers deploy directly to K8S
			Opentofu: immutable state Terraform vs OpenTofu
			
	Install
		Wsl2: c:\Users\<user>\.wslconfig
			memory=10GB processors=4 swap=2GB size=80
		AWS: 12GB RAM
		OVH:
		
	Probable next steps - use cases
		install - Play
		Have your own RAG database - if not in my DB RAG go to OpenAI
		Agents MCP 
		From lab to PROD - Tech change personal view
		gitOps - ArgoCD
		MLOps - MLFlow or Kubeflow or both
		
	Q&A - contribute
		share your experience or thoughts
		any help with your lab
		but do not be a hater
	